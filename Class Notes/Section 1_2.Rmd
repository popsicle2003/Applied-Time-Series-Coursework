---
title: An R Markdown document converted from "Section 1_2.ipynb"
output: html_document
---

# 1.2 Time Series Statistical Models

## 1.2.1 Basic Concepts

Like in other subjects of statistics (e.g., linear regression), a key component for statistical data analysis is: *statistical model*.

**Definition** A *time series statistical model* is a mathematical model which describes how sample time series data is generated over time.

Since we mainly deal with time series data exhibiting random fluctuation, we may use random variables to model our observations.

Say at time $t=1$, we model the data to be observed by a random variable $x_1$; at $t=2$, we model by $x_2$; at $t=3$, we model by $x_3$; etc.

**Definition** A collection of random variables $\{x_t\}$ indexed by a time index $t$ is called a *stochastic process*. When $t$ is discrete, we say $\{x_t\}$ is a discrete-time stochastic process; when $t$ is continuous, we say $\{x_t\}$ is a continuous-time stochastic process.

**Key**:

-   The random variables $x_t$ at different $t$'s may be statistically dependent.
-   The statistical characteristics of $\{x_t\}$ may change as time progresses.

Contrast: independent and identically distributed (iid) data.

**Continuous Time vs Discrete Time**

-   Many time series could have been observed at any continuous point in time, and are conceptually more properly treated as continuous-time process.

-   However, sampled data is always discrete because of restrictions inherent in the method of collection.

-   Selection of the sampling interval or rate can significantly impact the observed data. (In signal processing, there is systematic study of such impact, e.g., [Nyquistâ€“Shannon sampling theorem](https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem)).

In time series analysis (at least in this course), we only focus on *discrete-time stochastic processes*, and typically ignore the issue with sampling.

(However, this is not to say continuous-time modeling is not important.)

Common discrete $t$ index set: $\mathbb{Z}_+=\{1,2,3,\ldots\}$, $\mathbb{N}=\{0,1,2,3,\ldots\}$, $\mathbb{Z}=\{\ldots,-2,-1,0,1,2,\ldots\}$.

**Definition** A sequence of realized (non-random) values of the stochastic process is referred to as a *realization* of the process.

In this course, the term *time series* may refer to either the *unrealized (random) model*, or the *realized (non-random) data*. One needs to look at the context to distinguish them.

**Time Series** vs **Stochastic Processes** as subjects of study.

-   **Stochastic Processes**: a subject in probability theory; emphasizes on studying properties of samples generated by various stochastic process models;

-   **Time Series**: a subject in statistics; emphasizes on inferring the models which are assumed to generate the observed sample.

In addition, Stochastic Processes tend to emphasize more on continuous time.

Models in Time Series are often simpler and less "physical" compared to models studied in Stochastic Processes.

That being said, these two subjects are intimately connected.

## 1.2.2 White Noise

Concept Review: let $X$ and $Y$ be random variables.

**Expectation (expected value, mean)**:

$$
\begin{equation}
E(X)= \begin{cases}
\sum_{x} x p(x) &  \text{discrete $X$ with probability mass function $p(x)$}\\
\int xf(x)dx & \text{continuous $X$ with probability density function $f(x)$}
\end{cases}
\end{equation}
$$

**Variance** $$
\begin{equation}
Var(X)=E(|X-E(X)|^2)=E(X^2)-(E(X))^2
\end{equation}
$$

**Covariance**

$$
\begin{equation}
Cov(X,Y)=E[(X-E(X))(Y-E(Y)) ]
\end{equation}
$$ **Correlation** $$\begin{equation}
Corr(X,Y)= \frac{Cov(X,Y)}{\sqrt{{Var(X)}{Var(Y)}}}
\end{equation} (\text{if the variances in the denominator are nonzero.})
$$

**Definition** A time series $\{w_t\}$ is said to be *white noise*, if

-   $E(w_t)=0$ for all $t$;
-   $Var(w_t)=\sigma_w^2$ for all $t$, $\sigma_w^2\in (0,\infty)$;
-   $Cov(w_s,w_t)=0$ for all $s\neq t$.

Namely, white noise is a centered, constant-variance and uncorrelated sequence of random variables.

Often we require a stronger notion (recall *independence* $\Rightarrow$ uncorrelatedness):

**Definition** A time series $\{w_t\}$ is said to be *white independent noise* , or simply *iid noise*, if $\{w_t\}$ are iid random variables with mean $0$ and finite nonzero variance $\sigma_w^2$.

Sometimes we assume normality in addition.

**Definition** A time series $\{w_t\}$ is said to be *Gaussian white noise*, if $\{w_t\}$ are iid $N(0,\sigma_w^2)$ random variables.

Summary: $\{\text{Gaussian white noise}\}$ $\subset$ $\{\text{white independent  noise}\}$ $\subset$ $\{\text{white noise}\}$.

**Note**: there is no consensus on the meaning of "white noise" (outside our course). It may mean either of the three. Check the meaning in a specific context.

See <https://en.wikipedia.org/wiki/White_noise> for more information.

```{r}
w=as.ts(rnorm(1000)) # generate Gaussian white noise, mean 0, variance 1
plot(w)
```

```{r}
acf(w)
```

Visual feature of white noise: rough irregular fluctuation.

**Exercise 7**. There exists time series model which is white noise but not white independent noise.

Rarely a time series is captured simply by a white noise model. White noise is often used as a building block of more sophisticated models.

## 1.2.2 Moving Average and Linear Filter

Time series in real life often exhibits temporal correlation.

Here is a simple idea to create a model with temporal correlation.

Suppose $\{w_t\}$ is a standard Gaussian white noise time series.

Set $$
v_t = \frac{1}{3}(w_{t-1}+w_{t}+w_{t+1}),
$$

namely, $v_t$ replaces $w_t$ by an average of its current value and its immediate neighbors in the past and future.

We call such $\{v_t\}$ a *moving average* of $\{w_t\}$.

One sees a correlation between $v_t$ and $v_{t+1}=\frac{1}{3}(w_{t}+w_{t+1}+w_{t+2})$, since they share the terms $w_t$ and $w_{t+1}$. (Will compute the correlation precisely later.)

```{r}
rep(1/3,3)
```

```{r}
w = rnorm(500,0,1) # generate 500 N(0,1) random numbers
v = filter(w, sides=2, filter=rep(1/3,3)) # moving average, will elaborate on the function "filter" below.

plot.ts(w, ylim=c(-3,3), main="white noise")
plot.ts(v, ylim=c(-3,3), main="moving average")
```

Observations

-   The moving average $\{v_t\}$ tends to be smoother compared to white noise.
-   Still have zero mean and constant variance.

**Definition.** Given a sequence of numbers $\{x_t\}$, a linear (time-invariant) filter is an operation on $\{x_t\}$ which yields a new sequence of the form $$
y_t=\sum_{i} a_{t-i} x_i
$$ where $\{a_{i}\}$ is a given sequence of coefficients. This mathematical operation between $\{x_t\}$ and $\{a_t\}$ is called a *convolution.*

Mathematically, one can understand a linear filter $\mathcal{A}$ represented by the coefficient sequence $\{a_{i}\}$ as a mapping $\{x_t\} \overset{\cal{A}}{\longrightarrow} \{y_t\}$.

**Remark**:

In practice, only a finite number of $a_{i}$ coefficients are nonzero.

In theoretical development, one may allow infinitely many nonzero $a_{i}$ as long as the infinite sum converges.

**Example.** The moving average $\{w_t\}\rightarrow \{\frac{1}{3}w_{t-1}+\frac{1}{3}w_t+\frac{1}{3}w_{t+1} \}$ can be represented by a linear filter with coefficients: $$
a_{1}=a_0=a_{-1}=\frac{1}{3},  \quad a_i =0 \text{ for other }i.
$$

**Example.** The lag-$\ell$ differencing operation $\{x_t\}\rightarrow \{x_t-x_{t-l}\}$ can be represented by a linear filter $\mathcal{A}$ with coefficient specification $$
a_0=1, \  a_{\ell}=-1,   \quad a_i =0 \text{ for other }i.
$$

**Remark**\*: one may more generally discuss a linear filter of the form $y_t=\sum_{i} a_{t,i} x_i.$ The specification $a_{t,i} = a_{t-i}$ imposes a time-invariance property of the filter, and is more commonly used due to its simplicity.

The R function which implements linear filters $\mathcal{A}$ is *filter* (in base Stats package, do not confuse with the *filter* function in *dplyr*).

```         
filter(x, filter, method = c("convolution", "recursive"),
       sides = 2, circular = FALSE, init)
x
a univariate or multivariate time series.

filter
a vector of filter coefficients in reverse time order (as for AR or MA coefficients).

sides
for convolution filters only.
If sides = 1 the filter coefficients are for past values only;
if sides = 2 they are centred around lag 0.
In this case the length of the filter should be odd,
but if it is even, more of the filter is forward in time than backward.
```

Recall the linear filter operation:

$$
y_t=\sum_{i} a_{t-i} x_i
$$

Here is how R function *filter* implements the linear filter:

Let $n$ be the length of the *filter* vector as an input variable to the *filter* function.

If sides = 1, then $a_0$=filter[1], $a_1$=filter[2], ..., $a_{n-1}$=filter[n], $a_i=0$ for other $i$.

$$y_t=a_0x_t+a_1 x_{t-1}+\ldots + a_{n-1}x_{t-n+1}$$

If sides = 2 and $n$ is odd, then $a_{-(n-1)/2}$=filter[1], ..., $a_0$= filter[(n+1)/2], ..., $a_{(n-1)/2}$=filter[n], $a_i=0$ for other $i$.

$$y_t=a_{-(n-1)/2}x_{t+(n-1)/2}+\ldots+ a_0 x_t +... +a_{(n-1)/2}x_{t-(n-1)/2}$$

```{r}
x=c(1,2,3,4,5)
filter(x,sides=1,c(1,1,1))
filter(x,sides=2,c(1,1,1))
```

With a proper choice of a linear filter, one can roughen/sharpen a time series instead of smoothing it.

```{r}
w = rnorm(500) # 500 N(0,1) variates
v = filter(w, sides=2, filter=c(1/2,-1,1/2)) # a "sharpening" linear filter
#par(mfrow=c(2,1))
plot.ts(w, main="white noise")
plot.ts(v, ylim=c(-3,3), main="sharpened")
```

## 1.2.3 Autoregressions and Random Walks

Moving average builds temporal correlation by filtering white noise.

Another arguably more explicit way to build temporal correlation is through *autoregression*.

Let $\{w_t\}$ be standard Gaussian white noise. Consider

$$
\begin{equation}
x_t=x_{t-1}-0.9 x_{t-2} + w_t,  \quad t=1,2,3, \ldots \tag{1}
\end{equation}
$$

with some initial values for $x_0,x_{-1}$.

**Note**

-   The model (1) is nothing but a multiple linear regression model with response variable $x_t$ and explanatory variables $x_{t-1}$ and $x_{t-2}$.
-   However, unlike the usual linear regression, a data point may play both the roles of response and explanatory variables.

```{r}
w = rnorm(1000)
x = filter(w, filter=c(1,-0.9), method="recursive")
plot.ts(x, main="autoregression")
```

**Observation**: The iteration generates relatively stable fluctuations around a constant level (zero).

**Note**: Sometimes a stretch of initial time points are dropped in the simulation to ensure the sample path has (approximately) entered an stable state.

Search "*equilibrium of Markov processes*" to learn more.

Is the stabilization phenomenon always happening? What if the model is instead (dropping the lag-2 variable) $$
\begin{equation}
x_t=x_{t-1} + w_t, \quad t=1,2,3,\ldots   \tag{2}
\end{equation}  \text{ with an initial value $x_0$}.
$$

```{r}
w = rnorm(10000)
x = filter(w, filter=1 , method="recursive")
plot.ts(x, main="random walk")
```

Observation: The fluctuation seems no longer stable and exhibits random trends.

The model (2) above is known as a *random walk*. Iterating the relation (2), one can derive $$
x_t=x_0+w_1+\ldots + w_t.
$$

One can also add a deterministic trend into the random walk. For example,\
$$
x_t=0.2+x_{t-1} + w_t
$$ Iterating the relation one can derive $$
x_t=x_0 + 0.2t + w_1+\ldots +w_t,
$$often called a *random walk with drift*.

```{r}
w = rnorm(1000)
wd = w + 0.2
xd = filter(wd, filter=c(1), method="recursive")
plot.ts(xd, main="random walk with drift")
abline(a=0, b=.2, lty=2) #lty=2 for dashed line
```

Alternatively, the simulation above can be carried out by "cumsum" function in R.

```{r}
cumsum(c(2,1,3,1,0))
```

```{r}
w = rnorm(1000)
wd = w +.2
xd = cumsum(wd)
plot.ts(xd, main="random walk with drift")
abline(a=0, b=.2, lty=2)
```

**Definition.** An (causal linear) autoregression time series model is given in the form

$$
x_t = a_0+ a_1 x_{t-1}+ \ldots + a_p x_{t-p} + w_t
$$ where is $w_t$ is white noise, and certain initial values are specified for the first $p$ time points (sometimes one may omit specifying initial values).

**Note.** The generation of such sequence can be implemented through "filter" function in R by specifying "method" as "recursive".

```         
filter(x, filter, method = c("convolution", "recursive") , circular = FALSE, init)

method   
Either "convolution" or "recursive" (and can be abbreviated).
If "convolution" a moving average is used;
if "recursive" an autoregression is used.

init
for recursive filters only.
Specifies the initial values of the time series just prior to the start value, in reverse time order. The default is a set of zeros.
```

**Exercise 8.** Write a wrapper function for simulating an arbitrary autoregression model.

We will study the moving average and the autoregression models (and their combination) in depth in Chapter 3.

## 1.2.4 Signal in Noise

Suppose still $(w_t)$ is Gaussian white noise (variance to be specified).

Consider the following model: $$
x_t=2\cos\left(2\pi \frac{t+15}{50} \right)+w_t
$$

```{r}
cs = 2*cos(2*pi*1:500/50 + .6*pi); w = rnorm(500,0,1)
#par(mfrow=c(3,1), mar=c(3,2,2,1), cex.main=1.5) # mar: margin size; cex.main: main title size
plot.ts(cs, main=expression(2*cos(2*pi*t/50+.6*pi)))
plot.ts(cs+w, main=expression(2*cos(2*pi*t/50+.6*pi) + N(0,1)))
plot.ts(cs+5*w, main=expression(2*cos(2*pi*t/50+.6*pi) + N(0,25)))
```

**Observation**: as the variance of the white noise increases, the siginal shape becomes obscured.

**Definition**. The ratio of the strength (e.g., the amplitude) of the signal to the strength (e.g., variance) of the noise is called the signal-to-noise ratio (SNR).

The large SNR , the easier it is to detect the signal.

SNR does not have a universal definition.

**Definition**. A signal-plus-noise model is given by $$
x_t=s_t+v_t
$$

$s_t$: the nonrandom signal (often smooth and unknown)

$v_t$: a zero-mean noise time series (may or may not be white/uncorrelated).

**Exercise 9**. An exploration of a combination of the time series models addressed so far.
