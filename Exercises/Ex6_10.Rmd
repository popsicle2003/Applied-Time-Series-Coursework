---
title: "Exercises 6-10"
output:
  html_document:
    df_print: paged
---

# Exercise 6

Complete Exercise 1.1 in the textbook. (You may need to study Examples 1.6 and 1.7 in the textbook first.)

```{r}
library(astsa)

ts.plot(EQ5,EXP6,col=c("blue","red"),lty=c(1,2),
        main = 'Earthquake(Blue) vs Explosion(Red)',
        ylab = "Seismic Amplitude")
legend("topleft", legend = c("Earthquake", "Explosion"),
       col = c("blue","red"), lty = c(1,2))

```

When we plot the earthquake along with the explosions, a few key differences become apparent.

1.  Firstly, we observe that both signals show two distinct arrival phases, with the P wave starting at around $t$ = 1, while the S wave is around 1050-1100.
2.  We also notice that the amplitude ratio between both the waves differ greatly for the earthquake and the explosion. For the earthquake, the ratio of amplitudes is much larger, and the S wave is far more sustained, when compared to the explosion amplitudes.
3.  Circling back to the sustained amplitude for the earthquake, this implies a higher frequency characteristic, leading to a sustained coda (tail) in comparison to an impulsive event like an earthquake.

# Exercise 7

In this exercise, you will explore an example of white but dependent noise.

Suppose $\{w_t\}_{t\in \mathbb{Z}_+}$ is a standard Gaussian white noise series, namely, i.i.d. standard normal random variables. Construct a time series $\{x_t\}_{t\in \mathbb{Z}_+ }$ through products of two consecutive $w_t$ values as follows: $x_1= w_1 w_2, x_2=w_2w_3, ... , x_t=w_{t}w_{t+1}$.

1.  Simulate and plot a realization of the time series $\{x_1,x_2,\ldots, x_{1000}\}$ as well as $\{w_1,\ldots,w_{1000}\}$. Apply the R function `acf` to the two series. Do the two `acf` plots look similar?

2.  Repeat Part 1 for the squared series $\{x_1^2,\ldots, x_{1000}^2\}$ and $\{w_1^2,\ldots,w_{1000}^2\}$ instead. What do you observe now?

*The next question is optional and will NOT be part of homework grade. You are encouraged to attempt it if you have taken STAT 4510.*

3\*. Mathematically argue that that $\{x_t\}$ constructed above is white noise, but NOT white *independent* noise. (Hint: if random variables are independent, then so should be their squares.)

**Part 1.**

```{r}
set.seed(42)

w <- rnorm(1001)
x <- w[1:1000] * w[2:1001]

w_trim <- w[1:1000]

par(mfrow=c(2,1)) # Stack plots
plot.ts(w_trim, main="Standard Gaussian White Noise (w)", col="blue")
plot.ts(x, main="Product Series (x)", col="red")

par(mfrow=c(2,1))
acf(w_trim, main="ACF of w (White Noise)")
acf(x, main="ACF of x (Product Series)")
```

We see that the ACF plots look very similar. Both $w_t$ and $x_t$ show negligible autocorrelations, confirming that $x_t$ is likely white noise.

**Part 2.**

```{r}
# Squaring the series
w_sq <- w_trim^2
x_sq <- x^2

par(mfrow=c(2,1))
acf(w_sq, main="ACF of w^2 (Squared White Noise)")
acf(x_sq, main="ACF of x^2 (Squared Product Series)")
```

Although it is hard to see, we observe that in the second plot, there is a rather large spike in comparison to the rest of the plot, which we do not observe in the first plot. This shows that while $x_t$ is uncorrelated, it is not independent, as $x_t$ influences the value of $x_{t+1}$ because they share a common factor $w_{t+1}$.

**Part 3.**

To demonstrate that $\{x_t\}$ is white noise, we must show that it has a mean of zero and no autocorrelation for lags $h \neq 0$.

**1. Mean Stationarity:** Since $w_t$ are independent standard normal variables with $E[w_t] = 0$: $$
E[x_t] = E[w_t w_{t+1}] = E[w_t]E[w_{t+1}] = 0 \cdot 0 = 0
$$

**2. Absence of Serial Correlation:** Consider the autocovariance function $\gamma_x(h) = E[x_t x_{t+h}] - E[x_t]E[x_{t+h}]$ for a lag $h \neq 0$. Since the means are zero: $$
\gamma_x(h) = E[x_t x_{t+h}] = E[w_t w_{t+1} w_{t+h} w_{t+h+1}]
$$

-   If $h \neq 0$, the set of indices $\{t, t+1\}$ and $\{t+h, t+h+1\}$ are disjoint (or share only one index).
-   Because the $w_t$ are independent, the expectation of the product factors into expectations of individual $w$ terms (or non-overlapping pairs).
-   Since $E[w_i] = 0$, any lone $w$ term makes the entire product zero.

Thus, $\gamma_x(h) = 0$ for all $h \neq 0$. Since the series has zero mean, finite variance, and zero autocorrelation, it satisfies the definition of **White Noise**.

If $\{x_t\}$ were a sequence of independent random variables, then any function of $x_t$ and $x_{t+1}$ must also be uncorrelated. Specifically, their squares $x_t^2$ and $x_{t+1}^2$ should be uncorrelated.

We test the covariance of the squared series at lag 1: $$
\text{Cov}(x_t^2, x_{t+1}^2) = E[x_t^2 x_{t+1}^2] - E[x_t^2]E[x_{t+1}^2]
$$

**Step A: Calculate the Expectation of the Product** $$
\begin{aligned}
E[x_t^2 x_{t+1}^2] &= E[(w_t w_{t+1})^2 (w_{t+1} w_{t+2})^2] \\
&= E[w_t^2 \cdot w_{t+1}^4 \cdot w_{t+2}^2]
\end{aligned}
$$ By independence of $w$, this separates into: $$
E[w_t^2] \cdot E[w_{t+1}^4] \cdot E[w_{t+2}^2]
$$ For a standard normal distribution, the second moment $E[Z^2] = 1$ and the fourth moment $E[Z^4] = 3$. Therefore: $$
E[x_t^2 x_{t+1}^2] = 1 \cdot 3 \cdot 1 = 3
$$

**Step B: Calculate the Product of Expectations** $$
E[x_t^2] = E[w_t^2]E[w_{t+1}^2] = 1 \cdot 1 = 1
$$ $$
E[x_t^2]E[x_{t+1}^2] = 1 \cdot 1 = 1
$$

**Conclusion:** $$
\text{Cov}(x_t^2, x_{t+1}^2) = 3 - 1 = 2 \neq 0
$$ Since the squares of consecutive observations are correlated, $x_t$ and $x_{t+1}$ are **dependent**, despite being uncorrelated themselves.

# Exercise 8

In the lecture we learned the autoregression model $x_t = a_0+ a_1 x_{t-1}+ \ldots + a_p x_{t-p} + w_t$

(1) Write an R function "GenAR" which simulates autoregression model with Gaussian white noise. You need to achieve the following:

-   The input of the function must include: length of simulated time series (sample size), variance of Gaussian white noise, the coefficient vector $(a_0,a_1,\ldots, a_p)$ and the initial value vector $(x_{-p+1},x_{-p+2},\ldots, x_0)$.

-   The output is a vector (preferably a *ts* object) containing the simulated time series.

You may directly make use of "filter" function in R.

(2) Use the function to simulate the following model $$
    x_t-2x_{t-1}+x_{t-2}= w_t,\quad t=1,2,3,\ldots, 1000
    $$ where $w_t$ is Gaussian white noise with variance equal to 2 , and initial values are set as $x_{-1}=1, x_{0}=0$.

(3) Discuss briefly what you observe. Does the time series curve look smoother or rougher compared to the random walk time series you saw in lecture?

(4) Difference (with lag 1) the simulated time series and plot the resulting time series. What do you observe this time?

# Exercise 9

Exercise 1.3 in the textbook.

# Exercise 10

Exercise 1.4 in the textbook.
